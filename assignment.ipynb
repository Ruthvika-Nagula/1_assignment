{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO6Pzf53l6ufwI+chBLDr/o",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ruthvika-Nagula/1_assignment/blob/main/assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZQ_ty7WjzhC",
        "outputId": "4f53a5ad-8eb2-46dd-9423-4090f64a91a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.33.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2025.6.15)\n",
            "Enter your Hugging Face API token:··········\n",
            "Enter your question: what is capital of india\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Answer:\n",
            "The capital city of India is New Delhi. Delhī (New Delhi) is a city and a union\n",
            "territory that serves as the seat of government of India and also the executive,\n",
            "legislative, and judiciary. The city is commonly referred to as Delhi or New\n",
            "Delhi, and it is strategically located in the northern part of the country.\n",
            "Delhi is frequently considered as a symbol of modern India as a result of it\n",
            "being the site of important governance institutions, governmental buildings, and\n",
            "diplomatic centers. Its official name, New Delhi, was derived from the name of\n",
            "its founder, New Delhi Development Authority (NDSA) that was set up in 1948 to\n",
            "plan, develop, and implement a new capital city on the site of the former\n",
            "British colonial capital of New Delhi, which remains India's capital to date.\n",
            "Delhi is administered as a separate territory under the constitution of India,\n",
            "although functions almost as a city-state and comes under the administration of\n"
          ]
        }
      ],
      "source": [
        "#question 1\n",
        "# Understand and explore the capabilities of Large Language Models (LLMs).\n",
        "# 1) Load and query any pre-trained LLM (Mistral or zephyr)\n",
        "# using Hugging Face Transformers.\n",
        "# 2) Generate text given a prompt.\n",
        "\n",
        "#Step 1: Install the required package\n",
        "!pip install huggingface_hub\n",
        "\n",
        "\n",
        "#Step 2: Import required modules\n",
        "from huggingface_hub import InferenceClient\n",
        "from getpass import getpass\n",
        "import textwrap\n",
        "\n",
        "#Step 3: Get your HF token\n",
        "HF_TOKEN = getpass(\"Enter your Hugging Face API token:\")\n",
        "\n",
        "#Step 4: Create an inference client\n",
        "client = InferenceClient(         #InferenceClient- Python class from the huggingface_hub\n",
        "                         model=\"HuggingFaceH4/zephyr-7b-beta\",\n",
        "                                  token=HF_TOKEN\n",
        ")\n",
        "\n",
        "#Step 5: Prepare your conversation (as a chat)\n",
        "user_question = input(\"Enter your question: \")\n",
        "\n",
        "messages = [\n",
        "    {\"role\":\"system\", \"content\": \"You are a helpful and knowledgeable assistant.\"},\n",
        "    {\"role\": \"user\", \"content\" : user_question}\n",
        "]\n",
        "\n",
        "#Step 6: Send the chat message\n",
        "response = client.chat_completion(messages=messages, max_tokens=200)\n",
        "\\\n",
        "#Step 7: Display the answer\n",
        "print(\"\\nAnswer:\")\n",
        "print(textwrap.fill(response.choices[0].message.content.strip(),width=80))                           #textwrap.fill() - converts it to readable format\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# question 2\n",
        "# Prompt Engineering\n",
        "# Analyze how different prompt formats affect output quality.\n",
        "# 1) Pick a simple LLM (EX: FLAN-T5).\n",
        "# 2) Create tasks like generation, summarization, Q&A, translation.\n",
        "# 3) Try few-shot, zero-shot, and chain-of-thought prompting.\n",
        "# 4) Compare outputs and mention short analysis in the form of comment lines.\n",
        "\n",
        "#  answering and summarizing\n",
        "\n",
        "from transformers import pipeline\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "# (1) Pipelines\n",
        "qa = pipeline(\"question-answering\", model=\"distilbert-base-uncased-distilled-squad\")\n",
        "summariser = pipeline(\"text2text-generation\", model=\"google/flan-t5-small\")\n",
        "\n",
        "# (2) Real context\n",
        "ctx = \"\"\"\n",
        "LangChain is an open‑source framework that helps developers build applications with large language\n",
        "models (LLMs). It provides modular components—chains, agents, document loaders, retrievers, memory,\n",
        "and vector stores—that make it easy to connect LLMs to external data and tools and to compose\n",
        "complex workflows.\n",
        "\"\"\"\n",
        "\n",
        "# (3) Ask the question\n",
        "question = \"What is LangChain?\"\n",
        "answer = qa(question=question, context=ctx)[\"answer\"]\n",
        "\n",
        "# Prompt to get a yes/no answer with a short explanation\n",
        "prompt = f\"Answer with Yes or No and explain briefly:\\n\\n{question}\"\n",
        "\n",
        "# (4) Summarise clearly\n",
        "summary_prompt = f\"Q: {question}\\nA: {answer}\\nNow summarise clearly.\"\n",
        "summary = summariser(summary_prompt, max_new_tokens=64)[0][\"generated_text\"]\n",
        "\n",
        "print(\"Extracted answer →\", answer)\n",
        "print(\"Final (summarised) answer →\", summary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-oMLx73m4Gk",
        "outputId": "fd78d02e-3215-4528-8e77-b001d9aceec4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted answer → an open‑source framework\n",
            "Final (summarised) answer → LangChain is an opensource framework.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# question 2\n",
        "# yes or no and summarizing\n",
        "\n",
        "# Step 1: Load FLAN-T5 model\n",
        "flan = pipeline(\"text2text-generation\",model=\"google/flan-t5-small\")\n",
        "\n",
        "# Step 2: Create your own PromptTemplate\n",
        "my_prompt = PromptTemplate.from_template(\"Please answer the following question with Yes or No and explain briefly:\\n\\nQuestion: {question}\")\n",
        "\n",
        "# Step 3: Define your input\n",
        "qs_input = {\"question\": \"Is it good?\"}\n",
        "\n",
        "# Step 4: Format the input using the prompt template\n",
        "formatted_input = my_prompt.format(**qs_input)\n",
        "\n",
        "# Step 5: Run the model\n",
        "response = flan(formatted_input,max_new_tokens=50)\n",
        "\n",
        "print(\"Final Answer:\\n\",response[0]['generated_text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ungiWNs_rXfm",
        "outputId": "82c6e629-d423-4f45-cf97-5c5d624758cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Answer:\n",
            " No\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# question 2\n",
        "# grammer correction yes or no and summarizing\n",
        "\n",
        "# Step 1: Load FLAN-T5 model\n",
        "flan = pipeline(\"text2text-generation\",model=\"google/flan-t5-small\")\n",
        "\n",
        "# Step 2: Create your own PromptTemplate\n",
        "my_prompt = PromptTemplate.from_template(\"Please answer the following question by correcting the grammar and explain briefly:\\n\\nQuestion: {question}\")\n",
        "\n",
        "# Step 3: Define your input\n",
        "qs_input = {\"question\": \"I were late to college\"}\n",
        "\n",
        "# Step 4: Format the input using the prompt template\n",
        "formatted_input = my_prompt.format(**qs_input)\n",
        "\n",
        "# Step 5: Run the model\n",
        "response = flan(formatted_input,max_new_tokens=50)\n",
        "\n",
        "print(\"Final Answer:\\n\",response[0]['generated_text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PClDDoevwuJh",
        "outputId": "f4a0f06b-9124-47ae-a4fc-40a7fb5ce6f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Answer:\n",
            " I was late to college.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# few shot q&a\n",
        "\n",
        "from transformers import pipeline\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "# Step 1: Load FLAN-T5 model\n",
        "flan = pipeline(\"text2text-generation\", model=\"google/flan-t5-small\")\n",
        "\n",
        "# Step 2: Few-shot prompt template with placeholders\n",
        "prompt_template = PromptTemplate.from_template(\"\"\"\n",
        "Answer the following questions briefly.\n",
        "\n",
        "Q: Is the Earth round?\n",
        "A: Yes, the Earth is round.\n",
        "\n",
        "Q: Can humans breathe underwater without equipment?\n",
        "A: No, humans cannot breathe underwater without equipment.\n",
        "\n",
        "Q: Is LangChain an open-source framework for building applications with large language models?\n",
        "A: Yes, LangChain helps build LLM-based applications.\n",
        "\n",
        "Q: {question}\n",
        "A:\"\"\")\n",
        "\n",
        "# Step 3: Define your input question\n",
        "input_question = {\"question\": \"Is Python a programming language?\"}\n",
        "\n",
        "# Step 4: Format the prompt\n",
        "formatted_prompt = prompt_template.format(**input_question)\n",
        "\n",
        "# Step 5: Run the model\n",
        "response = flan(formatted_prompt, max_new_tokens=50)\n",
        "\n",
        "# Step 6: Output the answer\n",
        "print(\"Q:\", input_question[\"question\"])\n",
        "print(\"A:\", response[0]['generated_text'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-RvqqRBzyK9Y",
        "outputId": "95207ff9-8fb1-4451-c77d-030d876339df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q: Is Python a programming language?\n",
            "A: Python is a programming language.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Translation\n",
        "from transformers import MarianMTModel, MarianTokenizer     #MarianMT is optimized for translation tasks\n",
        "\n",
        "\n",
        "# Step 1: Load the English → Telugu translation model\n",
        "flan = pipeline(\"text2text-generation\",model=\"Helsinki-NLP/opus-mt-en-hi\")\n",
        "\n",
        "my_prompt = PromptTemplate.from_template(\"Translate this to Hindi:\\n\\n{sentence}\")\n",
        "\n",
        "# Step 2: Define your input text\n",
        "content_input = {\"sentence\": \"How are you?\"}\n",
        "\n",
        "formatted_input = my_prompt.format(**content_input)\n",
        "response = flan(formatted_input, max_new_tokens = 150)\n",
        "print(\"Sentence: \\n\",response[0]['generated_text'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JykJVY6ho4sd",
        "outputId": "b6868529-ae2e-4722-bb22-44b4b8198ff7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence: \n",
            " इसे हिन्दी अनुवाद करें: आप कैसे हैं?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Install the deep-translator library\n",
        "\n",
        "!pip install deep-translator\n",
        "\n",
        "# Step 2: Import the translator\n",
        "from deep_translator import GoogleTranslator\n",
        "\n",
        "# Step 3: Define the input sentence\n",
        "english_text = \"How are you?\"\n",
        "\n",
        "# Step 4: Translate from English to Telugu\n",
        "translated_text = GoogleTranslator(source='en', target='te').translate(english_text)\n",
        "\n",
        "# Step 5: Print the output\n",
        "print(\"Original Text:\", english_text)\n",
        "print(\"Translated to Telugu:\", translated_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a-CHbRhAwYgG",
        "outputId": "9c42f9b5-c7f4-450f-e751-0ebb2b8c7bc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting deep-translator\n",
            "  Downloading deep_translator-1.11.4-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in /usr/local/lib/python3.11/dist-packages (from deep-translator) (4.13.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from deep-translator) (2.32.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep-translator) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep-translator) (4.14.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2025.6.15)\n",
            "Downloading deep_translator-1.11.4-py3-none-any.whl (42 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: deep-translator\n",
            "Successfully installed deep-translator-1.11.4\n",
            "Original Text: How are you?\n",
            "Translated to Telugu: మీరు ఎలా ఉన్నారు?\n"
          ]
        }
      ]
    }
  ]
}